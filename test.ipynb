{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "test.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNgsA/XaQ+B4dM6gSpu4tfr",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jjbmsda/ThesisStudy/blob/main/test.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RSLQg-q__qyj",
        "outputId": "95dc991e-3833-4c0f-945c-2f2b0519f4b0"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_Nht1KjzBq3H",
        "outputId": "827be88f-4597-479a-b7f2-3565c5ccc04a"
      },
      "source": [
        "%cd /content/drive/MyDrive/DmmlTiSV-master"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/MyDrive/DmmlTiSV-master\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 232
        },
        "id": "cGPY1aluCE_t",
        "outputId": "62ad039e-3784-41b8-d9de-8a005237bc2a"
      },
      "source": [
        "import os\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\"\n",
        "import io\n",
        "import requests\n",
        "\n",
        "from PIL import Image\n",
        "from torchvision import models\n",
        "from torch.autograd import Variable\n",
        "from torch.nn import functional as F\n",
        "import numpy as np\n",
        "#import cv2\n",
        "import pdb\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import glob\n",
        "import random\n",
        "import numpy as np\n",
        "import time\n",
        "import math\n",
        "import sys\n",
        "import scipy.io as sio\n",
        "from sklearn import *\n",
        "import matplotlib.pyplot as plt\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils import data\n",
        "from tqdm import tqdm\n",
        "from config import Config\n",
        "from sklearn import *\n",
        "from sklearn import metrics as Metrics\n",
        "from models import resnet\n",
        "from torch.nn import DataParallel\n",
        "\n",
        "import librosa\n",
        "from scipy.signal.windows import hamming\n",
        "import soundfile as sf\n",
        "\n",
        "SAMPLE_RATE = 16000\n",
        "FEATURE = 'fft'#\n",
        "FEATURE_LEN = 161#\n",
        "WIN_LEN = 0.02#\n",
        "WIN_STEP = 0.01#\n",
        "\n",
        "N_FFT = int(WIN_LEN * SAMPLE_RATE)#\n",
        "HOP_LEN = int(WIN_STEP * SAMPLE_RATE)#\n",
        "\n",
        "N_FRAMES = 300#\n",
        "DURATION = (N_FRAMES - 1) * WIN_STEP#\n",
        "N_SAMPLES = int(DURATION * SAMPLE_RATE)#\n",
        "\n",
        "N_TEST_FRAMES = 300#\n",
        "TEST_DURATION = (N_TEST_FRAMES - 1) * WIN_STEP#\n",
        "N_TEST_SAMPLES = int(TEST_DURATION * SAMPLE_RATE)#\n",
        "\n",
        "\n",
        "def load_audio(filename, start=0, stop=None, resample=True):#\n",
        "    sr = SAMPLE_RATE\n",
        "    y, sr = sf.read(filename, start=start, stop=stop, dtype='float32', always_2d=True)\n",
        "    y = np.squeeze(y)\n",
        "    return y, sr\n",
        "\n",
        "\n",
        "class my_dataset(data.Dataset):\n",
        "    def __init__(self, root, lists):\n",
        "        self.root = root\n",
        "        self.lists = lists\n",
        "\n",
        "        self.transforms = transforms.Compose([\n",
        "                transforms.ToTensor(),])\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        im_pth = self.lists[index]\n",
        "        im_pth = os.path.join(self.root, im_pth)\n",
        "        im, sr = load_audio(im_pth)\n",
        "        S = librosa.core.stft(im, n_fft=N_FFT, hop_length=HOP_LEN, window=hamming)#\n",
        "        feature, _ = librosa.magphase(S)\n",
        "        npy = np.log1p(feature)#\n",
        "        npy = npy.transpose()\n",
        "        npy = npy[np.newaxis, :, :]\n",
        "        im = np.repeat(npy, 3, axis=0)\n",
        "        #print(im.shape)\n",
        "\n",
        "\n",
        "\n",
        "        num_frame = 300\n",
        "        l = im.shape[1]\n",
        "        ims = []\n",
        "        for i in range(10):\n",
        "            \n",
        "            if l <= num_frame:\n",
        "                new = np.zeros((3, num_frame, 161))\n",
        "                new[:, :l, :] = im\n",
        "                new[:, num_frame-l:, :] = im[:, :l, :]\n",
        "                npy = new\n",
        "            else:\n",
        "                randint = np.random.randint(l - num_frame)\n",
        "                npy = im[:, randint: randint+num_frame, :]\n",
        "            #print(npy.shape)\n",
        "\n",
        "\n",
        "\n",
        "            npy = np.swapaxes(npy,1,2)\n",
        "            mu = np.average(npy)\n",
        "            sigma = np.std(npy)\n",
        "            npy = (npy - mu) / max(sigma, 0.001)\n",
        "            #print(npy.shape)\n",
        "            npy = self.transforms(npy).float()\n",
        "            npy = npy.permute(1,2,0)\n",
        "            npy = npy.unsqueeze(0)\n",
        "            #print(npy.shape)\n",
        "            if i == 0:\n",
        "                ims = npy\n",
        "            else:\n",
        "                ims = torch.cat((ims, npy), 0)\n",
        "\n",
        "        #print(ims.shape)\n",
        "\n",
        "\n",
        "        return ims, self.lists[index]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.lists)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#root_dir = \"/data2/xjw/vox1_test_npy/\"\n",
        "#test_path = \"/home/xjw/workspace/metric_learning_speaker_verification/veri_test.txt\"\n",
        "#path = \"/home/xjw/workspace/triplet-network-pytorch-master/veri_png.txt\"\n",
        "\n",
        "def get_lists(path):\n",
        "    files= []\n",
        "    f = open(path, \"r\")\n",
        "    lines = f.readlines()\n",
        "    for line in lines:\n",
        "        line = line.split()\n",
        "        #print(line[1], line[2])\n",
        "        if line[1] not in files:\n",
        "            files.append(line[1])\n",
        "        if line[2] not in files:\n",
        "            files.append(line[2])\n",
        "    return files\n",
        "\n",
        "def get_featurs(model, root_dir, lists):\n",
        "\n",
        "    #pbar = tqdm(total=len(lists))\n",
        "    #for idx, img_path in enumerate(lists):\n",
        "    #    pbar.update(1)\n",
        "    features_dict = {}\n",
        "\n",
        "    trainloader = my_dataset(root=root_dir, lists=lists)\n",
        "    dl = data.DataLoader(trainloader, batch_size=1)\n",
        "\n",
        "    #trainloader = data.DataLoader(lists, batch_size=1)\n",
        "    for i, (img, im_pth) in enumerate(dl):\n",
        "        if i % 100 == 0:\n",
        "            print(i)\n",
        "        img = img.cuda()\n",
        "        img = torch.squeeze(img)\n",
        "        #img = img.unsqueeze(1)\n",
        "        #print(i, img.shape, im_pth[0])\n",
        "        if i == 0:\n",
        "            _, feature = model(img)\n",
        "            feature = feature.detach().cpu().numpy()\n",
        "            features_dict[im_pth[0]] = feature \n",
        "            features = feature\n",
        "        else:\n",
        "            _, feature = model(img)\n",
        "            feature = feature.detach().cpu().numpy()\n",
        "            features_dict[im_pth[0]] = feature \n",
        "            features = np.concatenate((features, feature), axis=0)\n",
        "    return features, features_dict\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def cosin_metric(features1, features2):\n",
        "    score = np.mean(np.matmul(features1, features2.T))\n",
        "    #print(\"score:\", score)\n",
        "    return score\n",
        "\n",
        "\n",
        "\n",
        "#score = score_vector\n",
        "#label = target_label_vector\n",
        "\n",
        "\n",
        "def calculate_eer_auc_ap(label,distance):\n",
        "\n",
        "    fpr, tpr, thresholds = Metrics.roc_curve(label, distance, pos_label=1)\n",
        "    AUC = Metrics.roc_auc_score(label, distance, average='macro', sample_weight=None)\n",
        "    AP = Metrics.average_precision_score(label, distance, average='macro', sample_weight=None)\n",
        "\n",
        "    # Calculating EER\n",
        "    intersect_x = fpr[np.abs(fpr - (1 - tpr)).argmin(0)]\n",
        "    EER = intersect_x\n",
        "    miss = 1 - fpr\n",
        "    false = 1 - tpr\n",
        "    #false = 1 - fpr\n",
        "    #miss = 1 - tpr\n",
        "    #C_det = 0.01*\n",
        "\n",
        "    return EER,AUC,AP,fpr, tpr, miss, false\n",
        "\n",
        "# K-fold validation for ROC\n",
        "\n",
        "\n",
        "\n",
        "def vox_test(model, test_root, test_list):\n",
        "\n",
        "    lists = get_lists(test_list)\n",
        "    print(len(lists))\n",
        "\n",
        "    f = open(test_list, \"r\")\n",
        "    lines = f.readlines()\n",
        "    score_vector = np.zeros((len(lines), 1))\n",
        "    target_label_vector = np.zeros((len(lines), 1))\n",
        "    features, features_dict = get_featurs(model, test_root, lists)\n",
        "\n",
        "    lists = np.array(lists)\n",
        "\n",
        "    scores = []\n",
        "    labels = []\n",
        "\n",
        "    for i, line in enumerate(lines):\n",
        "        line = line.split()\n",
        "        labels.append(int(line[0]))\n",
        "        score = cosin_metric(features_dict[line[1]], features_dict[line[2]])\n",
        "        #print(score)\n",
        "        scores.append(score)    \n",
        "\n",
        "\n",
        "    score = np.array(scores)[:, np.newaxis]\n",
        "    label = np.array(labels)[:, np.newaxis]\n",
        "\n",
        "#score_vector = softmax.softmax(score_vector)\n",
        "\n",
        "#np.save(os.path.join('/home/xjw/workspace/metric_learning_speaker_verification/result/','score_vector.npy'),score)\n",
        "#np.save(os.path.join('/home/xjw/workspace/metric_learning_speaker_verification/result/','target_label_vector.npy'),label)\n",
        "    k = 1\n",
        "    step = int(label.shape[0] / float(k))\n",
        "    EER_VECTOR = np.zeros((k,1))\n",
        "    AUC_VECTOR = np.zeros((k,1))\n",
        "    C_det = np.zeros((k,1))\n",
        "    for split_num in range(k):\n",
        "        index_start = split_num * step\n",
        "        index_end = (split_num + 1) * step\n",
        "    #print(label[index_start:index_end])\n",
        "    #print(score[index_start:index_end])\n",
        "        EER_temp,AUC_temp,AP,fpr, tpr, miss, false = calculate_eer_auc_ap(label[index_start:index_end],score[index_start:index_end])\n",
        "        EER_VECTOR[split_num] = EER_temp * 100\n",
        "        AUC_VECTOR[split_num] = AUC_temp * 100\n",
        "        C_det[split_num] = 10 *0.01* np.mean(miss) + 1 * 0.99 * np.mean(false)\n",
        "\n",
        "    eer = np.mean(EER_VECTOR)\n",
        "    print(\"eer:\", eer)\n",
        "    return eer\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "\n",
        "    opt = Config()\n",
        "    #model = resnet34()\n",
        "\n",
        "\n",
        "    logs_root = opt.logs_root\n",
        "    logs = os.listdir(logs_root)\n",
        "\n",
        "    model = resnet.resnet50(pretrained=False,num_classes=5994).cuda()\n",
        "    model = nn.DataParallel(model)\n",
        "    model.cuda()\n",
        "    #model.load_state_dict(torch.load(\"./logs/model_avgpool2.pth.tar\"))\n",
        "\n",
        "    model.eval()\n",
        "    vox_test(model, opt.vox_root, opt.vox_test_list)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-a4891a538525>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    269\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    270\u001b[0m     \u001b[0mlogs_root\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogs_root\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 271\u001b[0;31m     \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogs_root\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    272\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    273\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresnet50\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpretrained\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnum_classes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5994\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/home/xjw/workspace/DmmlTiSV-master/logs/'"
          ]
        }
      ]
    }
  ]
}